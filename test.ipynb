{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://nhacnheo.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "songs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanhn\\AppData\\Local\\Temp\\ipykernel_3800\\3016634967.py:60: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  author_section = soup.find(text=lambda x: \"Sáng tác\" in x)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://hopamviet.vn/sheet/index/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Get the results\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_data_from_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Output the results (you can save this to a file or process it further)\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "Cell \u001b[1;32mIn[46], line 34\u001b[0m, in \u001b[0;36mextract_data_from_table\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Fetch and process details from the fourth link\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     title, author, tags, lyrics \u001b[38;5;241m=\u001b[39m fetch_details_from_link(fourth_link)\n\u001b[0;32m     33\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m---> 34\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond_link_images\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mimages\u001b[49m,\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfourth_link_title\u001b[39m\u001b[38;5;124m'\u001b[39m: title,\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfourth_link_author\u001b[39m\u001b[38;5;124m'\u001b[39m: author,\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfourth_link_tags\u001b[39m\u001b[38;5;124m'\u001b[39m: tags,\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfourth_link_lyrics\u001b[39m\u001b[38;5;124m'\u001b[39m: lyrics\n\u001b[0;32m     39\u001b[0m     })\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to fetch HTML content\n",
    "def fetch_html(url):\n",
    "    response = requests.get(url)\n",
    "    return response.content\n",
    "\n",
    "# Function to extract data from the main table\n",
    "def extract_data_from_table(url):\n",
    "    html_content = fetch_html(url)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    rows = soup.find('tbody').find_all('tr')\n",
    "    results = []\n",
    "\n",
    "    for row in rows:\n",
    "        # Get the 2nd <td> link\n",
    "        second_td = row.find_all('td')[1]\n",
    "        second_link = second_td.find('a')['href']\n",
    "\n",
    "        # Get the 4th <td> link\n",
    "        fourth_td = row.find_all('td')[3]\n",
    "        fourth_link = fourth_td.find('a')['href']\n",
    "\n",
    "        # Fetch and process the images from the second link\n",
    "        images = fetch_images_from_link(second_link)\n",
    "\n",
    "        # Fetch and process details from the fourth link\n",
    "        title, author, tags, lyrics = fetch_details_from_link(fourth_link)\n",
    "\n",
    "        results.append({\n",
    "            'second_link_images': images,\n",
    "            'fourth_link_title': title,\n",
    "            'fourth_link_author': author,\n",
    "            'fourth_link_tags': tags,\n",
    "            'fourth_link_lyrics': lyrics\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to fetch images from a link\n",
    "def fetch_images_from_link(url):\n",
    "    html_content = fetch_html(url)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    image_tags = soup.select('div.ct-box img')\n",
    "    images = [img['src'] for img in image_tags]\n",
    "    return images\n",
    "\n",
    "# Function to fetch title, author, tags, and lyrics from a link\n",
    "def fetch_details_from_link(url):\n",
    "    html_content = fetch_html(url)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract title\n",
    "    title = soup.find('h1', class_='h3 d-inline-block').text.strip()\n",
    "\n",
    "    # Extract author\n",
    "    author_section = soup.find(text=lambda x: \"Sáng tác\" in x)\n",
    "    author = \"\"\n",
    "    if author_section:\n",
    "        author = author_section.split(\"Sáng tác:\")[1].split('|')[0].strip()\n",
    "\n",
    "    # Extract tags\n",
    "    tags_section = soup.find('i', class_='fas fa-book')\n",
    "    tags = \"\"\n",
    "    if tags_section:\n",
    "        tags = tags_section.find_next('a').text.strip()\n",
    "\n",
    "    # Extract lyrics\n",
    "    lyrics_div = soup.find('div', id='lyric')\n",
    "    if lyrics_div:\n",
    "        # for span in lyrics_div.find_all('span'):\n",
    "        #     span.extract()\n",
    "        # Delete all the <span></span> content inside lyrics_div\n",
    "        [span.extract() for span in lyrics_div.find_all('span')]\n",
    "        \n",
    "        lyrics = lyrics_div.text.strip()\n",
    "\n",
    "    return title, author, tags, lyrics\n",
    "\n",
    "# Main URL\n",
    "url = \"https://hopamviet.vn/sheet/index/\"\n",
    "\n",
    "# Get the results\n",
    "data = extract_data_from_table(url)\n",
    "\n",
    "# Output the results (you can save this to a file or process it further)\n",
    "for item in data:\n",
    "    print(\"Title:\", item['fourth_link_title'])\n",
    "    print(\"Author:\", item['fourth_link_author'])\n",
    "    print(\"Tags:\", item['fourth_link_tags'])\n",
    "    print(\"Lyrics:\", item['fourth_link_lyrics'])\n",
    "    print(\"Images:\", item['second_link_images'])\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Link: https://hopamviet.vn/sheet/song/chi-anh-moi-hieu/W8IU9WWD.html\n",
      "Fourth Link: https://hopamviet.vn/chord/song/chi-anh-moi-hieu/W8IU9WWD.html\n",
      "Files: ['https://hopamviet.vn/assets/images/sheets/CHI-MINH-ANH-HIEUpdf.pdf']\n",
      "Title: Chỉ anh mới hiểu\n",
      "Author: Nguyễn Đình Chương\n",
      "Tags: Nhạc Trữ tình\n",
      "Lyrics:\n",
      " Cuộc tình theo gió cuốn về đâu cho thêm sầu\n",
      "Còn lại nỗi nhớ biết tìm em ở chốn nao\n",
      "Lời thề nguyện ước sẽ không xa rời nhau\n",
      "Anh vẫn nhớ em người yêu dấu thế nhưng em xa rồi\n",
      "\n",
      "Người ở nơi ấy có buồn không cho cuộc tình\n",
      "Cuộc tình tan vỡ hỏi tại sao lòng đắng cay\n",
      "Chờ hoài chẳng thấy bóng dáng em quay về đây\n",
      "Khi bóng đêm lạnh lùng.\n",
      "\n",
      "ĐK\n",
      "Người yêu ơi người biết không hạnh phúc anh chỉ cần có em\n",
      "Chỉ mình em và em thôi tình yêu ấy chỉ anh mới hiểu được tình anh\n",
      "Đến thế nào mà em ơi em còn nhớ về anh\n",
      "\n",
      "Người yêu ơi dù cách xa nhưng trái tim vẫn còn nhớ em\n",
      "Nhớ về em dẫu thế nào dù hai đứa hai lối về \n",
      "Tình vỡ chẳng còn thì anh vẫn \n",
      "Trọn đời mãi yêu em\n",
      "\n",
      "CODA\n",
      "Em dù ở đâu tình chúng ta cách chia đôi đường\n",
      "Biết bao lần gọi tên em cho vơi nhớ thương \n",
      "Nhưng mà đầy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanhn\\AppData\\Local\\Temp\\ipykernel_3800\\1039865819.py:85: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  author_text_node = author_section.find_next_sibling(text=True)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to fetch HTML content\n",
    "def fetch_html(url):\n",
    "    response = requests.get(url)\n",
    "    return response.content\n",
    "\n",
    "# Function to extract data from a specific <tr>\n",
    "def extract_data_from_row(url, row_index=0):\n",
    "    html_content = fetch_html(url)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all <tr> elements in the <tbody>\n",
    "    rows = soup.find('tbody').find_all('tr')\n",
    "    \n",
    "    if not rows or row_index >= len(rows):\n",
    "        print(f\"No <tr> found at index {row_index}.\")\n",
    "        return\n",
    "    \n",
    "    # Get the specified <tr>\n",
    "    row = rows[row_index]\n",
    "    \n",
    "    # Get the 2nd <td> link\n",
    "    second_td = row.find_all('td')[1]\n",
    "    second_link = second_td.find('a')['href']\n",
    "    print(\"Second Link:\", second_link)\n",
    "\n",
    "    # Get the 4th <td> link\n",
    "    fourth_td = row.find_all('td')[3]\n",
    "    fourth_link = fourth_td.find('a')['href']\n",
    "    print(\"Fourth Link:\", fourth_link)\n",
    "\n",
    "    # Fetch and process the images from the second link\n",
    "    # images = fetch_images_from_link(second_link)\n",
    "    files = fetch_files_from_link(second_link)\n",
    "    # print(\"Images:\", images)\n",
    "    print(\"Files:\", files)\n",
    "\n",
    "    # Fetch and process details from the fourth link\n",
    "    title, author, tags, lyrics = fetch_details_from_link(fourth_link)\n",
    "\n",
    "    print(\"Title:\", title)\n",
    "    print(\"Author:\", author)\n",
    "    print(\"Tags:\", tags)\n",
    "    print(\"Lyrics:\\n\", lyrics)\n",
    "\n",
    "# Function to fetch images and other files from a link\n",
    "def fetch_files_from_link(url):\n",
    "    html_content = fetch_html(url)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Fetch image sources from <img> tags\n",
    "    image_tags = soup.select('div.ct-box img')\n",
    "    images = [img['src'] for img in image_tags]\n",
    "    \n",
    "    # Fetch PDF or other file sources from <iframe> tags\n",
    "    iframe_tags = soup.select('div.ct-box iframe')\n",
    "    files = []\n",
    "    for iframe in iframe_tags:\n",
    "        src = iframe.get('src')\n",
    "        if src and \"google.com/viewer?url=\" in src:\n",
    "            # Extract the actual file URL from the Google Viewer link\n",
    "            file_url = src.split(\"url=\")[1].split(\"&\")[0]\n",
    "            files.append(file_url)\n",
    "    \n",
    "    # Combine images and files into a single list\n",
    "    files.extend(images)\n",
    "    \n",
    "    return files\n",
    "# Function to fetch title, author, tags, and lyrics from a link\n",
    "def fetch_details_from_link(url):\n",
    "    html_content = fetch_html(url)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract title\n",
    "    title = soup.find('h1', class_='h3 d-inline-block').text.strip()\n",
    "\n",
    "    # Extract author names\n",
    "    author_section = soup.find('i', title=\"Sáng tác\")\n",
    "    author = \"\"\n",
    "    if author_section:\n",
    "        # Get the entire author text including the links\n",
    "        author_text_node = author_section.find_next_sibling(text=True)\n",
    "        if author_text_node:\n",
    "            # Find all <a> tags before encountering the next \"|\" symbol\n",
    "            author_links = []\n",
    "            current = author_text_node.find_next_sibling('a')\n",
    "            while current and current.name == 'a':\n",
    "                author_links.append(current.text.strip())\n",
    "                current = current.find_next_sibling()\n",
    "                if isinstance(current, str) and '|' in current:\n",
    "                    break\n",
    "            author = ' & '.join(author_links)\n",
    "\n",
    "    # Extract tags\n",
    "    tags_section = soup.find('i', class_='fas fa-book')\n",
    "    tags = \"\"\n",
    "    if tags_section:\n",
    "        tags = tags_section.find_next('a').text.strip()\n",
    "\n",
    "    # Extract lyrics\n",
    "    lyrics_div = soup.find('div', id='lyric')\n",
    "    lyrics = lyrics_div.text.strip()\n",
    "    \n",
    "    # Remove all chords in the format [Am], [Dm], etc. using regex\n",
    "    lyrics = re.sub(r'\\[.*?\\] ', '', lyrics)\n",
    "    # FInd if in the lyrics there are any chords in the format [Am], [Dm], etc.\n",
    "    # chords = re.findall(r'\\[.*?\\]', lyrics)\n",
    "    # Remove all of them\n",
    "    lyrics = re.sub(r'\\[.*?\\]', '', lyrics)    \n",
    "\n",
    "    return title, author, tags, lyrics\n",
    "\n",
    "# Main URL\n",
    "# url = \"https://hopamviet.vn/sheet/index/\"\n",
    "url = \"https://hopamviet.vn/sheet/index/700\"\n",
    "\n",
    "# Test with a specific <tr> for debugging\n",
    "# For example, test with the second <tr> (index 1)\n",
    "extract_data_from_row(url, row_index=99)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
